{
 "metadata": {
  "name": "",
  "signature": "sha256:b4c3f8f0d97957cadccfd7962a3752de4add6fb653d7edbfbdb5f977000f00f2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Density estimation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The Iris Data Set\n",
      "\n",
      "The [Iris data set](http://archive.ics.uci.edu/ml/datasets/Iris) (often also called Fishers Iris Data) is a well known data set in data mining. The data set consists of 50 samples from each of three species of the Iris flower (Iris setosa, Iris virginica and Iris versicolor). For every flower four measures have been taken: the length and the width of the sepals and petals measured in centimetres (see also http://en.wikipedia.org/wiki/Iris_flower_data_set)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Downloading and importing the iris data set\n",
      "import numpy as np\n",
      "import urllib2 #need for web access\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.mlab import bivariate_normal\n",
      "import numpy.random as rnd\n",
      "from mpl_toolkits.mplot3d.axes3d import Axes3D\n",
      "from sklearn import cross_validation\n",
      "req = urllib2.urlopen('http://mlpy.sourceforge.net/docs/3.2/_downloads/iris1.csv')\n",
      "iris = np.loadtxt(req, delimiter=',')\n",
      "# data: (observations x attributes) matrix, classes: classes (1: setosa, 2: versicolor, 3: virginica)\n",
      "data, classes = iris[:, :4], iris[:, 4].astype(np.int) \n",
      "print \"Dataset Description: (Please note that in our version the last attribute is numeric to work with numpy)\"\n",
      "print \"=======================================================================================================\\n\"\n",
      "print urllib2.urlopen('http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.names').read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Dataset Description: (Please note that in our version the last attribute is numeric to work with numpy)\n",
        "=======================================================================================================\n",
        "\n",
        "1. Title: Iris Plants Database\n",
        "\tUpdated Sept 21 by C.Blake - Added discrepency information\n",
        "\n",
        "2. Sources:\n",
        "     (a) Creator: R.A. Fisher\n",
        "     (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
        "     (c) Date: July, 1988\n",
        "\n",
        "3. Past Usage:\n",
        "   - Publications: too many to mention!!!  Here are a few.\n",
        "   1. Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
        "      Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions\n",
        "      to Mathematical Statistics\" (John Wiley, NY, 1950).\n",
        "   2. Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
        "      (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
        "   3. Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
        "      Structure and Classification Rule for Recognition in Partially Exposed\n",
        "      Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
        "      Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
        "      -- Results:\n",
        "         -- very low misclassification rates (0% for the setosa class)\n",
        "   4. Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE \n",
        "      Transactions on Information Theory, May 1972, 431-433.\n",
        "      -- Results:\n",
        "         -- very low misclassification rates again\n",
        "   5. See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al's AUTOCLASS II\n",
        "      conceptual clustering system finds 3 classes in the data.\n",
        "\n",
        "4. Relevant Information:\n",
        "   --- This is perhaps the best known database to be found in the pattern\n",
        "       recognition literature.  Fisher's paper is a classic in the field\n",
        "       and is referenced frequently to this day.  (See Duda & Hart, for\n",
        "       example.)  The data set contains 3 classes of 50 instances each,\n",
        "       where each class refers to a type of iris plant.  One class is\n",
        "       linearly separable from the other 2; the latter are NOT linearly\n",
        "       separable from each other.\n",
        "   --- Predicted attribute: class of iris plant.\n",
        "   --- This is an exceedingly simple domain.\n",
        "   --- This data differs from the data presented in Fishers article\n",
        "\t(identified by Steve Chadwick,  spchadwick@espeedaz.net )\n",
        "\tThe 35th sample should be: 4.9,3.1,1.5,0.2,\"Iris-setosa\"\n",
        "\twhere the error is in the fourth feature.\n",
        "\tThe 38th sample: 4.9,3.6,1.4,0.1,\"Iris-setosa\"\n",
        "\twhere the errors are in the second and third features.  \n",
        "\n",
        "5. Number of Instances: 150 (50 in each of three classes)\n",
        "\n",
        "6. Number of Attributes: 4 numeric, predictive attributes and the class\n",
        "\n",
        "7. Attribute Information:\n",
        "   1. sepal length in cm\n",
        "   2. sepal width in cm\n",
        "   3. petal length in cm\n",
        "   4. petal width in cm\n",
        "   5. class: \n",
        "      -- Iris Setosa\n",
        "      -- Iris Versicolour\n",
        "      -- Iris Virginica\n",
        "\n",
        "8. Missing Attribute Values: None\n",
        "\n",
        "Summary Statistics:\n",
        "\t         Min  Max   Mean    SD   Class Correlation\n",
        "   sepal length: 4.3  7.9   5.84  0.83    0.7826   \n",
        "    sepal width: 2.0  4.4   3.05  0.43   -0.4194\n",
        "   petal length: 1.0  6.9   3.76  1.76    0.9490  (high!)\n",
        "    petal width: 0.1  2.5   1.20  0.76    0.9565  (high!)\n",
        "\n",
        "9. Class Distribution: 33.3% for each of 3 classes.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 77
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise 1: Visualisation of the Iris Data Set with histograms and scatter plots\n",
      "\n",
      "* First load the data set with the above code.\n",
      "* Create a single histogram-plot for every dimension using matplotlib\n",
      "* Create a 4x4 plot containing the four histograms side by side\n",
      "* Create a single histogram-plot containing all dimensions\n",
      "* Implement a function that creates a scatter plot matrix, where the main diagonal are either boxplots or histograms and classes.\n",
      "* Encode the class information with different colors in the scatter plot matrix\n",
      "\n",
      "In all visualisations above take care off apropriate labelling of axis and a apropriate titles of figures. If you created the most basic form of the above visualisations, try to change the appearance for easier identifying patterns.\n",
      "\n",
      "Also, try to answer the following questions:\n",
      "\n",
      "* What does the distribution of an attribute look like? Are the attribute values reasonable?\n",
      "* Which of the above presentations of histograms allows you to easier compare the distributions of the different attributes?\n",
      "* Which attributes seem to be most discriminative to differentiate the different flowers?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "titles = ['sepal length in cm', 'sepal width in cm',\n",
      "          'petal length in cm', 'petal width in cm']\n",
      "f = plt.figure()\n",
      "for hist in range(4):\n",
      "    ax = f.add_subplot (2,2,hist)\n",
      "    ax.set_title (titles[hist])\n",
      "    ax.set_xlabel ('cm')\n",
      "    ax.set_ylabel ('number of flowers')\n",
      "    plt.hist (data[:,hist], bins=30)\n",
      "plt.show ()\n",
      "\n",
      "df = pd.DataFrame(data, columns=['sepal length', 'sepal width', 'petal length', 'petal width'])\n",
      "pd.scatter_matrix (df, diagonal='kde', c=['#ff0000', '#00ff00', '#0000ff'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 98,
       "text": [
        "array([[<matplotlib.axes.AxesSubplot object at 0x00000000379666A0>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x0000000034E54208>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x0000000034FE2588>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x000000003517A978>],\n",
        "       [<matplotlib.axes.AxesSubplot object at 0x000000003507D908>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x000000003523ECC0>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x00000000354EB710>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x00000000353D65C0>],\n",
        "       [<matplotlib.axes.AxesSubplot object at 0x0000000035175B00>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x0000000035369320>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x000000003548C828>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x00000000351417F0>],\n",
        "       [<matplotlib.axes.AxesSubplot object at 0x0000000034FA5860>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x0000000037FA24E0>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x0000000038015C18>,\n",
        "        <matplotlib.axes.AxesSubplot object at 0x000000003818E780>]], dtype=object)"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Exercise 2: Kernel density estimation\n",
      "\n",
      "###1) Window function\n",
      "\n",
      "We start with implementing so-called *window function* $\\phi$:\n",
      "\n",
      "$\\phi(\\pmb u) = \\Bigg[ \\begin{array}{ll} 1 & \\quad |u_j| \\leq 1/2 \\; ;\\quad \\quad j = 1, ..., d \\\\\n",
      "0 & \\quad otherwise \\end{array} $  \n",
      "for a hypercube of unit length 1 centered at the coordinate system's origin. What this function basically does is assigning a value 1 to a sample point if it lies within 1/2 of the edges of the hypercube, and 0 if lies outside (note that the evaluation is done for all dimensions of the sample point).\n",
      "\n",
      "\n",
      "If we extend on this concept, we can define a more general equation that applies to hypercubes of any length $h_n$ that are centered at $\\pmb x'$:  \n",
      "\n",
      "$\\phi(\\pmb u) = \\phi \\bigg( \\frac{\\pmb x - \\pmb x'}{h_n} \\bigg)$, where we have taken $\\pmb u = \\Big( \\frac{\\pmb x - \\pmb x'}{h_n} \\Big)$.\n",
      "\n",
      "You need to implement window function with inputs:\n",
      "\n",
      "* center point\n",
      "* kernel type\n",
      "* window length"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###2) Kernel density estimator\n",
      "\n",
      "Based on the window function, that we defined in the section above, we can now formulate the kernel estimation with a hypercube kernel as follows:\n",
      "\n",
      "$p_n(\\pmb x) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} \\frac{1}{h^d} \\phi \\bigg( \\frac{\\pmb x - \\pmb x_i}{h_n} \\bigg)$.\n",
      "\n",
      "You need to implement the kernel density estimator as a function with inputs:\n",
      "\n",
      "* training set\n",
      "* window length\n",
      "* kernel type"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "###3) Application of estimator\n",
      "\n",
      "1. Implement generation of two-dimensional Gaussian distribution (using numpy)\n",
      "2. Estimate density using kernel density estimator: play with sample size and window length.\n",
      "3. Try to extend by\n",
      "   - using different kernel types,\n",
      "   - consider a mixture of two Gaussians (distribution with two peaks).\n",
      "4. Estimate one and two dimensional densities for Iris data set.\n",
      "5. Perform classification in three classes using Bayes' decision rule (object is decided to belong the class to which it belongs with ,aximum probability), try to reach best possible quality."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def window_func (x, center, window_length, kernel_type):\n",
      "    tmp = (x - center)/float(window_length)\n",
      "    if kernel_type=='hypercube':\n",
      "        if (np.abs(tmp) < 0.5).all():\n",
      "            return 1\n",
      "        else:\n",
      "            return 0\n",
      "    if kernel_type=='gaussian':\n",
      "        return (1/np.sqrt(2*np.pi))*np.exp(-np.dot(tmp.T,tmp)/2.0) \n",
      "        \n",
      "\n",
      "def estimate_dense (x, data, window_length, kernel_type):\n",
      "    est_dense = 0\n",
      "    for i in range(data.shape[0]):\n",
      "        est_dense += window_func (x, data[i,:], window_length=window_length, \n",
      "                                  kernel_type=kernel_type)/float(window_length)\n",
      "    return est_dense/float(data.shape[0])\n",
      "\n",
      "def compute_data_for_plot (inp, window_length, grid=(35,35), kernel_type='hypercube'):\n",
      "    min_x, max_x = min (inp[:, 0]), max (inp[:, 0])\n",
      "    min_y, max_y = min (inp[:, 1]), max (inp[:, 1])\n",
      "    x_grid = np.linspace (min_x, max_x, grid[0])\n",
      "    y_grid = np.linspace (min_y, max_y, grid[1])\n",
      "    x_for_plot, y_for_plot = np.meshgrid(x_grid, y_grid)\n",
      "    values = np.array([estimate_dense([x,y], inp, window_length=window_length, kernel_type=kernel_type) \n",
      "               for x,y in zip(np.ravel(x_for_plot), np.ravel(y_for_plot))])\n",
      "    values = values.reshape(x_for_plot.shape)\n",
      "    return x_for_plot, y_for_plot, values\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Bivariate normal distribution, hypercube kernel\n",
      "normal2d = rnd.multivariate_normal ([0,0], [[1,0],[0,1]], 2000)\n",
      "x_for_plot, y_for_plot, values = compute_data_for_plot (normal2d, 1)\n",
      "\n",
      "#Plot estimated density and error\n",
      "fig1 = plt.figure()\n",
      "true_values = bivariate_normal(x_for_plot, y_for_plot)\n",
      "ax = fig1.add_subplot (1, 2, 1, projection='3d')\n",
      "ax.set_title('Estimated density (normal distribution): hypercube kernel')\n",
      "ax.plot_surface(x_for_plot, y_for_plot, values, rstride=1, cstride=1, cmap=plt.cm.coolwarm,\n",
      "        linewidth=0, antialiased=False) \n",
      "ax = fig1.add_subplot(1, 2, 2, projection='3d')\n",
      "ax.set_title('Error')\n",
      "ax.plot_surface(x_for_plot, y_for_plot, abs(true_values-values), rstride=1, cstride=1,\n",
      "        linewidth=0, antialiased=False)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Mixture of two gaussians, hypercube kernel\n",
      "two_peaks = rnd.multivariate_normal ([0,0], [[1,0],[0,1]], 1000)\n",
      "two_peaks = np.vstack ((two_peaks, rnd.multivariate_normal ([4,0], [[1,0],[0,1]], 1000)))\n",
      "x_for_plot, y_for_plot, values = compute_data_for_plot (two_peaks, 1.4)\n",
      "\n",
      "#Plot estimated density\n",
      "fig2 = plt.figure()\n",
      "ax = fig2.add_subplot(1, 1, 1, projection='3d')\n",
      "ax.set_title('Estimated density (mixture of two gaussians): hypercube kernel')\n",
      "ax.plot_surface(x_for_plot, y_for_plot, values, rstride=1, cstride=1, cmap=plt.cm.coolwarm,\n",
      "        linewidth=0, antialiased=False)\n",
      "plt.show()\n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Bivariate normal distribution, gaussian kernel\n",
      "normal2d = rnd.multivariate_normal ([0,0], [[1,0],[0,1]], 2000)\n",
      "x_for_plot, y_for_plot, values = compute_data_for_plot (normal2d, 1.4, kernel_type='gaussian')\n",
      "\n",
      "#Plot estimated density\n",
      "fig3 = plt.figure()\n",
      "true_values = bivariate_normal(x_for_plot, y_for_plot)\n",
      "ax = fig3.add_subplot(1, 2, 1, projection='3d')\n",
      "ax.set_title('Estimated density (normal distribution): gaussian kernel')\n",
      "ax.plot_surface(x_for_plot, y_for_plot, values, rstride=1, cstride=1, cmap=plt.cm.coolwarm,\n",
      "        linewidth=0, antialiased=False)\n",
      "ax = fig3.add_subplot(1, 2, 2, projection='3d')\n",
      "ax.set_title('Error')\n",
      "ax.plot_surface(x_for_plot, y_for_plot, abs(true_values-values), rstride=1, cstride=1,\n",
      "        linewidth=0, antialiased=False)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Mixture of two gaussians, gaussian kernel\n",
      "two_peaks = rnd.multivariate_normal ([0,0], [[1,0],[0,1]], 1000)\n",
      "two_peaks = np.vstack ((two_peaks, rnd.multivariate_normal ([4,0], [[1,0],[0,1]], 1000)))\n",
      "x_for_plot, y_for_plot, values = compute_data_for_plot (two_peaks, 1.2, kernel_type='gaussian')\n",
      "\n",
      "#Plot estimated density\n",
      "fig2 = plt.figure()\n",
      "ax = fig2.add_subplot(1, 1, 1, projection='3d')\n",
      "ax.set_title('Estimated density (mixture of two gaussians): gaussian kernel')\n",
      "ax.plot_surface(x_for_plot, y_for_plot, values, rstride=1, cstride=1, cmap=plt.cm.coolwarm,\n",
      "        linewidth=0, antialiased=False)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Plot 2d (petal length, petal width) densities for all classes\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(data, classes, test_size=0.4, random_state=0)\n",
      "class1 = np.array ([X_train[i][2:] for i in range(y_train.shape[0]) if y_train[i]==1])\n",
      "class2 = np.array ([X_train[i][2:] for i in range(y_train.shape[0]) if y_train[i]==2])\n",
      "class3 = np.array ([X_train[i][2:] for i in range(y_train.shape[0]) if y_train[i]==3])\n",
      "\n",
      "#class1\n",
      "x_for_plot, y_for_plot, values = compute_data_for_plot (class1, 0.1, kernel_type='gaussian')\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(1, 3, 1, projection='3d')\n",
      "ax.set_title('Iris Setosa')\n",
      "ax.plot_surface(x_for_plot, y_for_plot, values, rstride=1, cstride=1, cmap=plt.cm.coolwarm,\n",
      "        linewidth=0, antialiased=False)\n",
      "\n",
      "#class2\n",
      "x_for_plot, y_for_plot, values = compute_data_for_plot (class2, 0.2, kernel_type='gaussian')\n",
      "ax = fig.add_subplot(1, 3, 2, projection='3d')\n",
      "ax.set_title('Iris Versicolour')\n",
      "ax.plot_surface(x_for_plot, y_for_plot, values, rstride=1, cstride=1, cmap=plt.cm.coolwarm,\n",
      "        linewidth=0, antialiased=False)\n",
      "\n",
      "#class3\n",
      "x_for_plot, y_for_plot, values = compute_data_for_plot (class3, 0.2, kernel_type='gaussian')\n",
      "ax = fig.add_subplot(1, 3, 3, projection='3d')\n",
      "ax.set_title('Iris Virginica')\n",
      "ax.plot_surface(x_for_plot, y_for_plot, values, rstride=1, cstride=1, cmap=plt.cm.coolwarm,\n",
      "        linewidth=0, antialiased=False)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Plot 1d (petal length) densities for all classes\n",
      "X_train, X_test, y_train, y_test = cross_validation.train_test_split(data, classes, test_size=0.4, random_state=0)\n",
      "class1 = np.array ([X_train[i][2] for i in range(y_train.shape[0]) if y_train[i]==1])\n",
      "class2 = np.array ([X_train[i][2] for i in range(y_train.shape[0]) if y_train[i]==2])\n",
      "class3 = np.array ([X_train[i][2] for i in range(y_train.shape[0]) if y_train[i]==3])\n",
      "\n",
      "#class1\n",
      "x_for_plot, y_for_plot, values = compute_data_for_plot (class1, 0.1, kernel_type='gaussian')\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(1, 3, 1, projection='3d')\n",
      "ax.plot_surface(x_for_plot, y_for_plot, values, rstride=1, cstride=1, cmap=plt.cm.coolwarm,\n",
      "        linewidth=0, antialiased=False)\n",
      "\n",
      "#class2\n",
      "x_for_plot, y_for_plot, values = compute_data_for_plot (class2, 0.2, kernel_type='gaussian')\n",
      "ax = fig.add_subplot(1, 3, 2, projection='3d')\n",
      "ax.plot_surface(x_for_plot, y_for_plot, values, rstride=1, cstride=1, cmap=plt.cm.coolwarm,\n",
      "        linewidth=0, antialiased=False)\n",
      "\n",
      "#class3\n",
      "x_for_plot, y_for_plot, values = compute_data_for_plot (class3, 0.2, kernel_type='gaussian')\n",
      "ax = fig.add_subplot(1, 3, 3, projection='3d')\n",
      "ax.plot_surface(x_for_plot, y_for_plot, values, rstride=1, cstride=1, cmap=plt.cm.coolwarm,\n",
      "        linewidth=0, antialiased=False)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 76
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Classification by petal length and petal width\n",
      "ans = []\n",
      "for x in X_test[:,2:]:\n",
      "    densities = [estimate_dense (x, class1, 0.1, kernel_type='gaussian'), estimate_dense (x, class2, 0.2, kernel_type='gaussian'),\n",
      "                 estimate_dense (x, class3, 0.2, kernel_type='gaussian')]\n",
      "    ans.append (np.argmax(densities)+1)\n",
      "ans = np.array (ans)\n",
      "print np.count_nonzero (y_test!=ans)\n",
      "print y_test.shape[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "4\n",
        "60\n"
       ]
      }
     ],
     "prompt_number": 92
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}